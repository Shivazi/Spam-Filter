{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import comet_ml in the top of your file\n",
    "# from comet_ml import Experiment\n",
    "# # Add the following code anywhere in your machine learning file\n",
    "# experiment = Experiment(api_key=\"huBPMqNqRcOaPFvAQ8dvnkrsn\",\n",
    "#                         project_name=\"spam-filter\", workspace=\"shivazi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFSW Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import requests\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import base64\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model import OpenNsfwModel, InputType\n",
    "from image_utils import create_tensorflow_image_loader\n",
    "from image_utils import create_yahoo_image_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign In mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing in mail.....\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import email\n",
    "import smtplib\n",
    "import imaplib\n",
    "import getpass\n",
    "\n",
    "ORG_EMAIL   = \"@gmail.com\"\n",
    "print(\"Signing in mail.....\")\n",
    "email= \"shivazibiswas.ice.iu\"\n",
    "FROM_EMAIL  = email + ORG_EMAIL\n",
    "#FROM_PWD = getpass.getpass(\"PASSWORD : \")\n",
    "FROM_PWD=\"studyhardstaycool\"\n",
    "SMTP_SERVER = \"imap.gmail.com\"\n",
    "SMTP_PORT   = 993 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mail Body and Image Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import email\n",
    "mail = imaplib.IMAP4_SSL(SMTP_SERVER)\n",
    "mail.login(FROM_EMAIL,FROM_PWD)\n",
    "mail.select('inbox')\n",
    "\n",
    "type, data = mail.search(None, 'ALL')\n",
    "mail_ids = data[0]\n",
    "id_list = mail_ids.split()   \n",
    "\n",
    "latest_email_id = int(id_list[-1])\n",
    "typ,data=mail.fetch(str.encode(str(latest_email_id)), '(RFC822)' )\n",
    "\n",
    "for response_part in data:\n",
    "    if isinstance(response_part, tuple):\n",
    "        #getting mime message\n",
    "        msg=email.message_from_string(response_part[1].decode('utf-8'))\n",
    "        messageMainType = msg.get_content_maintype()\n",
    "        \n",
    "        html_data=\"\"\n",
    "        plain_data=\"\"\n",
    "        if messageMainType== 'multipart':\n",
    "            for part in msg.get_payload():\n",
    "                if part.get_content_subtype()=='html':\n",
    "                    html_data=html_data+part.get_payload()\n",
    "                elif part.get_content_subtype()=='plain':\n",
    "                    plain_data=plain_data+part.get_payload()\n",
    "        elif messageMainType=='text':\n",
    "            html_data=msg.get_payload()\n",
    "            plain_data=msg.get_payload()\n",
    "            \n",
    "\n",
    "email_subject = msg['subject']\n",
    "email_from = msg['from']\n",
    "email_to=msg['to']\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print ('From : ' + email_from + '\\n')\n",
    "# print ('To : ' + email_to + '\\n')\n",
    "# print ('Subject : ' + email_subject + '\\n')\n",
    "\n",
    "##Image extraction            \n",
    "soup = BeautifulSoup(html_data,\"lxml\")\n",
    "#print(plain_data)\n",
    "#print(soup)\n",
    "\n",
    "import quopri\n",
    "mystring = html_data\n",
    "mystring=mystring.encode('utf-8')\n",
    "decoded_string = quopri.decodestring(mystring)\n",
    "decoded_string=BeautifulSoup(decoded_string)\n",
    "#print(decoded_string)\n",
    "\n",
    "img_tags = decoded_string.find_all('img')\n",
    "#img_tags=soup.find_all('img')\n",
    "#print(img_tags)\n",
    "urls = [img['src'] for img in img_tags]\n",
    "#print(urls)\n",
    "\n",
    "for url in urls:\n",
    "    filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', url)\n",
    "    if filename!=None:\n",
    "        with open(filename.group(1), 'wb') as f:\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Png to Jpg Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = glob('./*.png')\n",
    "for j in pngs:\n",
    "    img = cv2.imread(j)\n",
    "    cv2.imwrite((j[:-3] + 'jpg' ), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load and Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenNsfwModel()\n",
    "model.build(weights_path= \"data/open_nsfw-weights.npy\", input_type=InputType.BASE64_JPEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_load_image = None\n",
    "fn_load_image = lambda filename: np.array([base64.urlsafe_b64encode(open(filename, \"rb\").read())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session And Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#All full black image delete\n",
    "# jpgs = glob('./*.jpg')\n",
    "# # print(jpgs)\n",
    "# for j in range(len(jpgs)):\n",
    "#     #print(jpgs[j])\n",
    "#     #print(os.path.getsize(jpgs[j]))\n",
    "#     #print(\"yes\")\n",
    "#     image = cv2.imread(jpgs[j], 0)\n",
    "#     if cv2.countNonZero(image) == 0:\n",
    "#         os.remove(jpgs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Nude_Image=0\n",
    "Non_Nude_Image=0\n",
    "with tf.Session() as sess:\n",
    "    jpgs = glob('./*.jpg')\n",
    "    for j in range(len(jpgs)):\n",
    "        if os.stat(jpgs[j]).st_size !=0:\n",
    "            input_file =jpgs[j]\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            image = fn_load_image(input_file)\n",
    "            predictions =sess.run(model.predictions,feed_dict={model.input: image})\n",
    "            #print(\"Results for '{}'\".format(input_file))\n",
    "            #print(\"\\tSFW score:\\t{}\\n\\tNSFW score:\\t{}\".format(*predictions[0]))\n",
    "            if((predictions[0][1])>.55):\n",
    "                Nude_Image=Nude_Image+1\n",
    "            else: Non_Nude_Image=Non_Nude_Image+1\n",
    "\n",
    "Extracted_Image=Nude_Image+Non_Nude_Image\n",
    "# print(\"\\n\")\n",
    "# print(\"Nude_Image: '{}'\" .format(Nude_Image))\n",
    "# print(\"Non_Nude_Image: '{}'\" .format(Non_Nude_Image))\n",
    "\n",
    "# if Nude_Image==0:\n",
    "#     print(\"Extracted Images from URL has been predicted as Hum\")\n",
    "# else: print(\"Extracted Images from URL has been predicted as Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = glob(os.path.join(\"*.png\"))\n",
    "filelist =filelist+ glob(os.path.join(\"*.jpg\"))\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malicious URL Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load from file\n",
    "pkl_filename=\"pickle_model_URL.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load from file\n",
    "pkl_filename=\"pickle_model_vectorizer.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:  \n",
    "    pickle_model_vectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURL(page):\n",
    "    \n",
    "    \"\"\"\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"a href\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"', start_link)\n",
    "    #print(start_quote)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    #print(end_quote)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = site\n",
    "#response = requests.get(url)\n",
    "#print(response)\n",
    "\n",
    "\n",
    "# parse html\n",
    "#print(decoded_string)\n",
    "#page = str(bs4.BeautifulSoup(decoded_string,\"lxml\"))\n",
    "page=str(decoded_string)\n",
    "bad_cnt=0\n",
    "good_cnt=0\n",
    "while True:\n",
    "    url, n = getURL(page)\n",
    "    #print(url)\n",
    "    page = page[n:]\n",
    "    if url:\n",
    "        if url.find(\"https\")==0:\n",
    "            url=url[8:]\n",
    "            url=[url]\n",
    "            #print(url)\n",
    "            X_predict = pickle_model_vectorizer.transform(url)\n",
    "            New_predict = pickle_model.predict(X_predict)\n",
    "            #print(New_predict)\n",
    "            if New_predict==\"bad\":\n",
    "                bad_cnt=bad_cnt+1\n",
    "            else: good_cnt=good_cnt+1\n",
    "        elif url.find(\"http\")==0 and url.find(\"https\")!=0:\n",
    "            url=url[7:]\n",
    "            url=[url]\n",
    "            #print(url)\n",
    "            #vectorizer.fit_transform(url)\n",
    "            X_predict = pickle_model_vectorizer.transform(url)\n",
    "            New_predict = pickle_model.predict(X_predict)\n",
    "            #print(New_predict)\n",
    "            if New_predict==\"bad\":\n",
    "                bad_cnt=bad_cnt+1\n",
    "            else: good_cnt=good_cnt+1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "Extracted_Url=bad_cnt+good_cnt\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Prediction_of_Reliable_URL: '{}' \" .format(good_cnt))        \n",
    "# print(\"Prediciton_of_Malicious_URL: '{}' \" .format(bad_cnt))\n",
    "\n",
    "# if bad_cnt>0:\n",
    "#     print(\"Extracted URL has been predicted as Spam\")\n",
    "# else: print(\"Extracted URL has been predicted as Hum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Spam Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencis\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in sorted(os.listdir(mail_dir))]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split() \n",
    "                    for word in words:\n",
    "                        wordID = 0 \n",
    "                        for i,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordID = i\n",
    "                                features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1     \n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model1_SVC\",'rb') as file1:\n",
    "    model1_SVC=pickle.load(file1)\n",
    "with open(\"model2_LNB\",'rb') as file2:\n",
    "    model2_LNB=pickle.load(file2)\n",
    "#Loading dictionary data\n",
    "with open('dictionary','rb') as file:\n",
    "    dictionary= pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import inscriptis\n",
    "# url=\"https://docs.python.org/3.8/about.html\"\n",
    "#print(\"Enter a URL..\")\n",
    "\n",
    "# url=site\n",
    "# html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "# text = inscriptis.get_text(html)\n",
    "\n",
    "import random\n",
    "if Extracted_Image==0 and Extracted_Url==0:\n",
    "    #write text file in a folder\n",
    "    textfile = open('/home/nybsys/Desktop/Deployment/Predicted_mail/textfile.txt', 'w')\n",
    "    textfile.write(plain_data)\n",
    "    textfile.close()\n",
    "\n",
    "else:\n",
    "    text=soup.text\n",
    "    #print(text)\n",
    "\n",
    "    words=text.split()\n",
    "    list_build=[]\n",
    "    for i in range(len(words)):\n",
    "        if len(words[i])<=3:\n",
    "            list_build.append(words[i]) \n",
    "    for i in range(len(list_build)):\n",
    "        words.remove(list_build[i])\n",
    "\n",
    "    #list to string generation\n",
    "    words=\" \".join(words)\n",
    "\n",
    "    #write text file in a folder\n",
    "    textfile = open('/home/nybsys/Desktop/Deployment/Predicted_mail/textfile.txt', 'w')\n",
    "    textfile.write(words)\n",
    "    textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the unseen mails for Spam\n",
    "test_dir = '/home/nybsys/Desktop/Deployment/Predicted_mail'\n",
    "test_matrix = extract_features(test_dir)\n",
    "#result1 = model1_SVC.predict(test_matrix)\n",
    "result2 = model2_LNB.predict(test_matrix)\n",
    "# if(result2==0):\n",
    "#     print(\"Extracted text has been predicted as Hum\")\n",
    "# else: print(\"Extracted text has been predicted as Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "From : Twitter <info@twitter.com>\n",
      "\n",
      "To : Shivazi Biswas <shivazibiswas.ice.iu@gmail.com>\n",
      "\n",
      "Subject : Shivazi Biswas, see 27 new updates from BBC Health News, Elon Musk,\r\n",
      " ...\n",
      "\n",
      "\n",
      "\n",
      "Prediction_of_Reliable_URL: '1' \n",
      "Prediciton_of_Malicious_URL: '8' \n",
      "\n",
      "\n",
      "Prediction of Non_Nude_Image: '12'\n",
      "Prediction of Nude_Image: '0'\n",
      "\n",
      "Extracted Image from URL has been predicted as Hum\n",
      "Extracted URL has been predicted as Spam\n",
      "Extracted text from URL has been predicted as Hum\n",
      "\n",
      "\n",
      "Final_Comment: Hum\n"
     ]
    }
   ],
   "source": [
    "#print(\"\\nTargeted URL...\")\n",
    "#print(site)\n",
    "\n",
    "print(\"\\n\")\n",
    "print ('From : ' + email_from + '\\n')\n",
    "print ('To : ' + email_to + '\\n')\n",
    "print ('Subject : ' + email_subject + '\\n')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction_of_Reliable_URL: '{}' \" .format(good_cnt))        \n",
    "print(\"Prediciton_of_Malicious_URL: '{}' \" .format(bad_cnt))\n",
    "\n",
    "print(\"\\n\")    \n",
    "print(\"Prediction of Non_Nude_Image: '{}'\" .format(Non_Nude_Image))\n",
    "print(\"Prediction of Nude_Image: '{}'\" .format(Nude_Image))\n",
    "\n",
    "if Nude_Image==0:\n",
    "    print(\"\\nExtracted Image from URL has been predicted as Hum\")\n",
    "else: print(\"\\nExtracted Image from URL has been predicted as Spam\")\n",
    "    \n",
    "if bad_cnt>good_cnt:\n",
    "    print(\"Extracted URL has been predicted as Spam\")\n",
    "else: print(\"Extracted URL has been predicted as Hum\")\n",
    "    \n",
    "if(result2==0):\n",
    "    print(\"Extracted text from URL has been predicted as Hum\")\n",
    "else: print(\"Extracted text from URL has been predicted as Spam\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "if Nude_Image==0 and result2==0:\n",
    "    print(\"Final_Comment: Hum\")\n",
    "else: print(\"Final_Comment: Spam\")\n",
    "# if Nude_Image==0 and bad_cnt<=good_cnt and result2==0:\n",
    "#     URL=\"http://120.50.14.28:8080/mail/send\"\n",
    "#     requests.post(URL,json={\"from\": email_from,\"to\": email_to,\"subject\": email_subject,\"msg\": plain_data})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
